\documentclass[../main/CT4S-EN-RU]{subfiles}

\begin{document}

\section{\caseENGRUS{Adjoint functors}{ / }{Сопряженные функторы}}

\begin{blockENG}
Just above, in the introduction to this chapter, I said that adjoint functors are like dictionaries that translate back and forth between different categories. How far can we take that analogy?
\end{blockENG}

\begin{blockRUS}
\end{blockRUS}

\begin{blockENG}
In the common understanding of dictionaries, we assume that the two languages (say French and English) are equally expressive, and that a good dictionary will be an even exchange of ideas. But in category theory we often have two categories that are not on the same conceptual level. This is most clear in the case of so-called {\em free-forgetful adjunctions}. In Section~\ref{sec:adjoints discuss and define} we will explore the sense in which each adjunction provides a dictionary between two categories that are not necessarily on an equal footing, so to speak.
\end{blockENG}

\begin{blockRUS}
\end{blockRUS}

%%%% Subsection %%%%

\subsection{\caseENGRUS{Discussion and definition}{ / }{Обсуждение и определение}}\label{sec:adjoints discuss and define}

\begin{blockENG}
Consider the category of monoids and the category of sets. A monoid $(M,e,{⋆})$ is a set with an identity element and a multiplication formula that is associative. A set is just a set. A dictionary between $\Mon$ and $\Set$ should not be required to set up an even exchange, but instead an exchange that is appropriate to the structures at hand. It will be in the form of two functors, one we'll denote by $L\colon\Set{→}\Mon,$ and one we'll denote by $R\colon\Mon{→}\Set.$ But to say what “appropriate” means requires more work.\index{a functor!$\Set{→}\Mon$}\index{a functor!$\Mon{→}\Set$}
\end{blockENG}

\begin{blockRUS}
\end{blockRUS}

\begin{blockENG}
Let's bring it down to earth with an analogy.\index{adjunction!analogy: babies and adults} A one-year-old can make repeatable noises and an adult can make repeatable noises. One might say “after all, talking is nothing but making repeatable noises.” But the adult's repeatable noises are called words, they form sentences, and these sentences can cause nuclear wars. There is something more in adult language than there is simply in repeatable sounds. In the same vein, a tennis match can be viewed as physics, but you won't see the match. So we have something analogous to two categories here: ((repeated noises)) and ((meaningful words)). We are looking for adjoint functors going back and forth, serving as the appropriate sort of dictionary.
\end{blockENG}

\begin{blockRUS}
\end{blockRUS}

\begin{blockENG}
To translate baby talk into adult language we would make every repeated noise a kind of word, thereby granting it meaning. We don't know what a given repeated noise should mean, but we give it a slot in our conceptual space, always pondering “I wonder what she means by Konnen..” On the other hand, to translate from meaningful words to repeatable noises is easy. We just hear the word as a repeated noise, which is how the baby probably hears it.
\end{blockENG}

\begin{blockRUS}
\end{blockRUS}

\begin{blockENG}
Adjoint functors often come in the form of “free” and “forgetful”. Here we freely add Konnen to our conceptual space without having any idea how it adheres to the rest of the child's noises or feelings. But it doesn't act like a sound to us, it acts like a word; we don't know what it means but we figure it means something. Conversely, the translation going the other way is “forgetful”, forgetting the meaning of our words and just hearing them as sounds. The baby hears our words and accepts them as mere sounds, not knowing that there is anything extra to get.
\end{blockENG}

\begin{blockRUS}
\end{blockRUS}

\begin{blockENG}
Back to sets and monoids, the sets are like the babies from our story: they are simple objects full of unconnected dots. The monoids are like adults, forming words and performing actions. In the monoid, each element means something and combines with other elements in some way. There are lots of different sets and lots of different monoids, just as there are many babies and many adults, but there are patterns to the behavior of each kind and we put them in different categories.
\end{blockENG}

\begin{blockRUS}
\end{blockRUS}

\begin{blockENG}
Applying free functor $L\colon\Set{→}\Mon$ to a set $X$ makes every element $x\in X$ a word, and these words can be strung together to form more complex words. (We discussed the free functor in Section~\ref{sec:free monoid}.) Since a set such as $X$ carries no information about the meaning or structure of its various elements, the free monoid $F(X)$ does not relate different words in any way. To apply the forgetful functor $R\colon\Mon{→}\Set$ to a monoid, even a structured one, is to simply forget that its elements are anything but mere elements of a set. It sends a monoid $(M,1,{⋆})$ to the set $M.$ 
\end{blockENG}

\begin{blockRUS}
\end{blockRUS}

\begin{blockENG}
The analogy is complete. However, this is all just ideas. Let's give a definition, then return to our sets, monoids, sounds, and words.
\end{blockENG}

\begin{blockRUS}
\end{blockRUS}

\begin{definitionENG}\label{def:adjunction}\index{adjunction}\index{functor!adjoint}\index{adjoint functors}
Let ${𝓑}$ and ${𝓐}$ be categories. \footnote{Throughout this definition, notice that $B$'s come before $A$'s, especially in (\ref{dia:adjunction isomorphism}), which might be confusing. It was a stylistic choice to match with the {\bf B}abies and {\bf A}dults discussion above and below this definition.}
An {\em adjunction between ${𝓑}$ and ${𝓐}$} is a pair of functors 
$$L\colon{𝓑}{→}{𝓐}\hsp\tn{and}\hsp R\colon{𝓐}{→}{𝓑}$$ 
together with a natural isomorphism
\footnote{The natural isomorphism $\alpha$ (see Lemma~\ref{lemma:natural iso}) is between two functors ${𝓑}\op\times{𝓐}{→}\Set,$ namely the functor $(B,A)\mapsto\Hom_{𝓐}(L(B),A)$ and the functor $(B,A)\mapsto\Hom_{𝓑}(B,R(A)).$} 
whose component for any objects $A\in\Ob({𝓐})$ and $B\in\Ob({𝓑})$ is: 
\begin{align}\label{dia:adjunction isomorphism}
\alpha_{B,A}\colon\Hom_{𝓐}(L(B),A)\Too{\iso}\Hom_{𝓑}(B,R(A)).
\end{align}
This isomorphism is called the {\em adjunction isomorphism} for the $(L,R)$ adjunction, and for any morphism $f\colon L(B){→} A$ in ${𝓐},$ we refer to $\alpha_{B,A}(f)\colon B{→} R(A)$ as {\em the adjunct} of $f.$
\footnote{Conversely, for any $g\colon B{→} R(A)$ in ${𝓑}$ we refer to $\alpha_{B,A}^{-1}(g)\colon L(B){→} A$ as {\em the adjunct} of $g.$}\index{adjunct}\index{adjunction!adjunction isomorphism}

The functor $L$ is called the {\em left adjoint} and the functor $R$ is called the {\em right adjoint}. We may say that {\em $L$ is the left adjoint of $R$} or that {\em $R$ is the right adjoint of $L$}. 
\footnote{The left adjoint does not have to be called $L,$ nor does the right adjoint have to be called $R,$ of course. This is suggestive.}
We often denote this setup by 
$$\adjoint{L}{{𝓑}}{{𝓐}}{R}$$
\end{definitionENG}

\begin{definitionRUS}\label{def:adjunction}\index{adjunction}\index{functor!adjoint}
\end{definitionRUS}

\begin{propositionENG}\label{prop:free forgetful monoid}
Let $L\colon\Set{→}\Mon$ be the functor sending $X\in\Ob(\Set)$ to the free monoid $L(X){\coloneqq}(\List(X),[\,],\plpl),$ as in Definition~\ref{def:free monoid}. Let $R\colon\Mon{→}\Set$ be the functor sending each monoid ${𝓜}{\coloneqq}(M,1,{⋆})$ to its underlying set $R({𝓜}){\coloneqq}M.$ Then $L$ is left adjoint to $R.$
\end{propositionENG}

\begin{propositionRUS}\label{prop:free forgetful monoid}
\end{propositionRUS}

\begin{proofENG}
If we can find a natural isomorphism of sets 
$$\alpha_{X,{𝓜}}\colon\Hom_\Mon(L(X),{𝓜}){→}\Hom_\Set(X,R({𝓜}))$$
we will have succeeded in showing that these functors are adjoint.

Suppose given an element $f\in\Hom_\Mon(L(X),{𝓜}),$ i.e. a monoid homomorphism $f\colon\List(X){→} M$ (sending $[\,]$ to $1$ and list concatenation to ${⋆}$). Then in particular we can apply $f$ to the singleton list $[x]$ for any $x\in X.$ This gives a function $X{→} M$ by $x\mapsto f([x]),$ and this is $\alpha_{X,{𝓜}}(f)\colon X{→} M=R({𝓜}).$ We need only to supply an inverse $\beta_{X,{𝓜}}\colon\Hom_\Set(X,R({𝓜})){→}\Hom_\Mon(L(X),{𝓜}).$

Suppose given an element $g\in\Hom_\Set(X,R({𝓜})),$ i.e. a function $g\colon X{→} M.$ Then to any list $\ell=[x_1,x_2,\ldots,x_n]\in\List(X)$ we can assign $\beta_{X,{𝓜}}(\ell){\coloneqq}g(x_1){⋆} g(x_2){⋆}\cdots{⋆} g(x_n)$ (if $\ell=[\,]$ is the empty list, assign $\beta_{X,{𝓜}}([\,]){\coloneqq}1$). We now have a function $\List(X){→} M.$ It is a monoid homomorphism because it respects identity and composition. It is easy to check that $\beta$ and $\alpha$ are mutually inverse, completing the proof.
\end{proofENG}

\begin{proofRUS}
\end{proofRUS}

\begin{exampleENG}
We need to ground our discussion in some concrete mathematics. In Proposition~\ref{prop:free forgetful monoid} we provided our long-awaited adjunction between sets and monoids. A set $X$ gets transformed into a monoid by considering lists in $X$; a monoid ${𝓜}$ gets transformed into a set by forgetting the multiplication law. So we have a functor going one way and the other, 
$$L\colon\Set{→}\Mon,\hspace{1in}R\colon\Mon{→}\Set,$$
but an adjunction is more than that: it includes a guarantee about the relationship between these two functors. What is the relationship between $L$ and $R?$ Consider an arbitrary monoid ${𝓜}=(M,1,*).$

If I want to pick out 3 elements of the set $M,$ that's the same thing as giving a function $\{a,b,c\}{→} M.$ But that function exists in the category of sets; in fact it is an element of $\Hom_\Set(\{a,b,c\},M).$ But since $M=R({𝓜})$ is the underlying set of our monoid, we can view the current paragraph in the light of our adjunction Equation (\ref{dia:adjunction isomorphism}) by saying it has been about the set
$$\Hom_\Set(\{a,b,c\},R({𝓜})).$$
This set classifies all the ways to pick three elements out of the underlying set of our monoid ${𝓜}.$ It was constructed completely from within the category $\Set.$

Now we ask what Equation (\ref{dia:adjunction isomorphism}) means. The equation
$$\Hom_\Mon(L(\{a,b,c\}),{𝓜})\iso\Hom_\Set(\{a,b,c\},R({𝓜})).$$
tells us that somehow we can answer the same question completely from within the category of monoids. In fact it tells us how to do so, namely as $\Hom_\Mon(\List(\{1,2,3\},{𝓜}).$  Exercise~\ref{ex:monoid adjunction} looks at how that should go. The answer is “hidden” in the proof of Proposition~\ref{prop:free forgetful monoid}.
\end{exampleENG}

\begin{exampleRUS}
\end{exampleRUS}

\begin{exerciseENG}\label{ex:monoid adjunction}
Let $X=\{a,b,c\}$ and let ${𝓜}=({ℕ},1,*)$ be the multiplicative monoid of natural numbers (see Example~\ref{ex:multiplication table}). Let $f\colon X{→}{ℕ}$ be the function given by $f(a)=7, f(b)=2, f(c)=2,$ and let $\beta_{X,{𝓜}}\colon\Hom_\Set(X,R({𝓜})){→}\Hom_\Mon(L(X),{𝓜})$ be as in the proof of Proposition~\ref{prop:free forgetful monoid}. What is $\beta_{X,{𝓜}}(f)([b,b,a,c])?$
\end{exerciseENG}

\begin{exerciseRUS}\label{ex:monoid adjunction}
\end{exerciseRUS}

\begin{blockENG}
Let us look once more at the adjunction between adults and babies. Using the notation of Definition~\ref{def:adjunction} ${𝓐}$ is the “adult category” of meaningful words and ${𝓑}$ is the “baby category” of repeated noises. The left adjoint turns every repeated sound into a meaningful word (having “free” meaning) and the right adjoint “forgets” the meaning of any word and considers it merely as a sound. 
\end{blockENG}

\begin{blockRUS}
\end{blockRUS}

\begin{blockENG}
At the risk of taking this simple analogy too far, let's have a go at the heart of the issue: how to conceive of the isomorphism (\ref{dia:adjunction isomorphism}) of $\Hom$'s. Once we have freely given a slot to each of baby's repeated sounds, we try to find a mapping from the lexicon $L(B)$ of these new words to our own lexicon $A$ of meaningful words; these are mappings in the adult category ${𝓐}$ of the form $L(B){→} A.$ And (stretching it) the baby tries to find a mapping (which we might see as emulation) from her set $B$ of repeatable sounds to the set $R(A)$ of the sounds the adult seems to repeat. If there was a global system for making these transformations that would establish  (\ref{dia:adjunction isomorphism}) and hence the adjunction.
\end{blockENG}

\begin{blockRUS}
\end{blockRUS}

\begin{blockENG}
Note that the directionality of the adjunction makes a difference. If $L\colon{𝓑}{→}{𝓐}$ is left adjoint to $R\colon{𝓐}{→}{𝓑}$ we rarely have an isomorphism $\Hom_{𝓐}(A,L(B))\iso\Hom_{𝓑}(R(A),B).$ In the case of babies and adults, we see that it would make little sense to look for a mapping in the category of meaningful words from the adult lexicon to the wordifications of baby-sounds, because there is unlikely to be a good candidate for most of our words. That is, to which of our child's repeated noises would we assign the concept “weekday”? 
\end{blockENG}

\begin{blockRUS}
\end{blockRUS}

\begin{blockENG}
Again, the above is simply an analogy, and almost certainly not formalizable. The next example shows mathematically the point we tried to make in the previous paragraph, that the directionality of an adjunction is not arbitrary.
\end{blockENG}

\begin{blockRUS}
\end{blockRUS}

\begin{exampleENG}\label{ex:adjunction monoids and sets}
Let $L\colon\Set{→}\Mon$ and $R\colon\Mon{→}\Set$ be the free and forgetful functors from Proposition~\ref{prop:free forgetful monoid}. We know that $L$ is left adjoint to $R$; however $L$ is {\em not} right adjoint to $R.$ In other words, we can show that the necessary natural isomorphism cannot exist.

Let $X=\{a,b\}$ and let ${𝓜}=(\{1\},1,!)$ be the trivial monoid. Then the necessary natural isomorphism would need to give us a bijection 
$$\Hom_\Mon({𝓜},L(X))\iso^?\Hom_\Set(\{1\},X).$$ 
But the left-hand side has one element, because ${𝓜}$ is the initial object in $\Mon$ (see Example~\ref{ex:initial monoid terminal monoid}), whereas the right-hand side has two elements. Therefore no isomorphism can exist.
\end{exampleENG}

\begin{exampleRUS}\label{ex:adjunction monoids and sets}
\end{exampleRUS}

\begin{exampleENG}
Preorders have underlying sets, giving rise to a functor $U\colon\PrO{→}\Set.$\index{a functor!$\PrO{→}\Set$} The functor $U$ has both a left adjoint and a right adjoint. The left adjoint of $U$ is $D\colon\Set{→}\PrO,$\index{a functor!$\Set{→}\PrO$} sending a set $X$ to the discrete preorder on $X$ (the preorder with underlying set $X,$ having the fewest possible $\leq$'s). The right adjoint of $U$ is $I\colon\Set{→}\PrO,$ sending a set $X$ to the indiscrete preorder on $X$ (the preorder with underlying set $X,$ having the most possible $\leq$'s). See Example~\ref{ex:discrete and indiscrete}. 
\end{exampleENG}

\begin{exampleRUS}
\end{exampleRUS}

\begin{exerciseENG}
Let $U\colon\Grph{→}\Set$\index{a functor!$\Grph{→}\Set$} denote the functor sending a graph to its underlying set of vertices. This functor has both a left and a right adjoint. 
\sexc What functor $\Set{→}\Grph$ is the left adjoint of $U?$
\item What functor $\Set{→}\Grph$ is the right adjoint of $U?$
\endsexc
\end{exerciseENG}

\begin{exerciseRUS}
\end{exerciseRUS}

\begin{exampleENG}\label{ex:other adjunctions}\index{currying!as adjunction}
Here are some other adjunctions:

\begin{itemize}
\item $\Ob\colon\Cat{→}\Set$\index{a functor!$\Ob\colon\Cat{→}\Set$} has a left adjoint $\Set{→}\Cat$ given by the discrete category.
\item $\Ob\colon\Cat{→}\Set$ has a right adjoint $\Set{→}\Cat$ given by the indiscrete category.
\item The underlying graph functor $\Cat{→}\Grph$\index{a functor!$\Cat{→}\Grph$}\index{a functor!$\Grph{→}\Cat$} has a left adjoint $\Grph{→}\Cat$ given by the free category.
\item The functor $\PrO{→}\Grph,$ \index{a functor!$\PrO{→}\Grph$} given by drawing edges for $\leq$'s, has a left adjoint given by existence of paths.
\item The forgetful functor from posets to preorders has a left adjoint given by quotient by isomorphism relation.
\item Given a set $A,$ the functor $(-\times A)\colon\Set{→}\Set$ has a right adjoint $\Hom(A,-)$ (this was called currying in Section~\ref{sec:currying}). 
\end{itemize}
\end{exampleENG}

\begin{exampleRUS}\label{ex:other adjunctions}\index{currying!as adjunction}
\end{exampleRUS}

\begin{exerciseENG}
Let $F\colon{𝓒}{→}{𝓓}$ and $G\colon{𝓓}{→}{𝓒}$ be mutually inverse equivalences of categories (see Definition~\ref{def:equiv of cats}). Are they adjoint in one direction or the other?
\end{exerciseENG}

\begin{exerciseRUS}
\end{exerciseRUS}

\begin{exerciseENG}
The discrete category functor $Disc\colon\Set{→}\Cat$ has a left adjoint $p\colon\Cat{→}\Set.$ 
\sexc For an arbitrary object $X\in\Ob(\Set)$ and an arbitrary object ${𝓒}\in\Ob(\Cat),$ write down the adjunction isomorphism.
\item Let ${𝓒}$ be the free category on the graph $G$:
$$
G{\coloneqq}\parbox{2in}{\fbox{\xymatrix{\LMO{v}\ar[r]^f&\LMO{w}\ar@/_1pc/[r]_h\ar@/^1pc/[r]^g&\LMO{x}\\\LMO{y}\ar@(l,u)[]^i\ar@/^1pc/[r]^j&\LMO{z}\ar@/^1pc/[l]^k}}}
$$
and let $X=\{1,2,3\}.$ How many elements does the set $\Hom_\Set({𝓒},Disc(X))$ have?
\item What can you do to an arbitrary category ${𝓒}$ to make a set $p({𝓒})$ such that the adjunction isomorphism holds? That is, how does the functor $p$ behave on objects?
\endsexc
\end{exerciseENG}

\begin{exerciseRUS}
\end{exerciseRUS}

\begin{blockENG}
The following proposition says that all adjoints to a given functor are isomorphic to each other. 
\end{blockENG}

\begin{blockRUS}
\end{blockRUS}

\begin{propositionENG}\label{prop:unicity of adjoints}
Let ${𝓒}$ and ${𝓓}$ be categories, let $F\colon{𝓒}{→}{𝓓}$ be a functor, and let $G,G'\colon{𝓓}{→}{𝓒}$ also be functors. If both $G$ and $G'$ are right adjoint (respectively left adjoint) to $F$ then there is a natural isomorphism $\phi\colon G{→} G'.$
\end{propositionENG}

\begin{propositionRUS}\label{prop:unicity of adjoints}
\end{propositionRUS}

\begin{proofENG}
Suppose that both $G$ and $G'$ are right adjoint to $F$ (the case of $G$ and $G'$ being left adjoint is similarly proved). We first give a formula for the components of $\phi\colon G{→} G'$ and its inverse $\psi\colon G'{→} G.$ Given an object $d\in\Ob({𝓓}),$ we use $c=G(d)$ to obtain two natural isomorphisms, one from each adjunction: 
$$\Hom_{𝓒}(G(d),G(d))\iso\Hom_{𝓓}(F(G(d)),d)\iso\Hom_{𝓒}(G(d),G'(d)).$$
The identity component $\id_{G(d)}$ is then sent to some morphism $G(d){→} G'(d),$ which we take to be $\phi_d.$ Similarly, we use $c'=G'(d)$ to obtain two natural isomorphisms, one from each adjunction:
$$\Hom_{𝓒}(G'(d),G'(d))\iso\Hom_{𝓓}(F(G'(d)),d)\iso\Hom_{𝓒}(G'(d),G(d)).$$
Again, the identity component $\id_{G'(d)}$ is sent to some morphism $G'(d){→} G(d),$ which we take to be $\psi_d.$ The naturality of the isomorphisms implies that $\phi$ and $\psi$ are natural transformations, and it is straightforward to check that they are mutually inverse.
\end{proofENG}

\begin{proofRUS}
\end{proofRUS}

%% Subsubsection %%

\subsubsection{\caseENGRUS{Quantifiers as adjoints}{ / }{Кванторы как сопряжения}}

\begin{blockENG}
One of the simplest but neatest places that adjoints show up is between preimages and the logical quantifiers $\exists$ and $\forall,$ which we first discussed in Notation~\ref{not:basic math notation}. \index{preimage}\index{a symbol!$\forall$}\index{a symbol!$\exists$} The setting in which to discuss this is that of sets and their power preorders. That is, if $X$ is a set then recall from Section~\ref{sec:meets and joins} that the power set ${ℙ}(X)$ has a natural ordering by inclusion of subsets. 
\end{blockENG}

\begin{blockRUS}
\end{blockRUS}

\begin{blockENG}
Given a function $f\colon X{→} Y$ and a subset $V\subseteq Y$ the preimage is $f^{-1}(V){\coloneqq}\{x\in X{\;|\;}f(x)\in V\}.$ If $V'\subseteq V$ then $f^{-1}(V')\subseteq f^{-1}(V),$ so in fact $f^{-1}\colon{ℙ}(Y){→}{ℙ}(X)$ can be considered a functor (where of course we are thinking of preorders as categories). The quantifiers appear as adjoints of $f^{-1}.$
\end{blockENG}

\begin{blockRUS}
\end{blockRUS}

\begin{blockENG}
Let's begin with the left adjoint of $f^{-1}\colon{ℙ}(Y){→}{ℙ}(X).$ It is a functor $L_f\colon{ℙ}(X){→}{ℙ}(Y).$ Choose an object $U\subseteq X$ in ${ℙ}(X).$ It turns out that
$$L_f(U)=\{y\in Y{\;|\;}\exists x\in f^{-1}(y)\tn{ such that }x\in U\}.$$
And the right adjoint $R_f\colon{ℙ}(X){→}{ℙ}(Y),$ when applied to $U$ is 
$$R_f(U)=\{y\in Y{\;|\;}\forall x\in f^{-1}(y), x\in U\}.$$
In fact, the functor $L_f$ is generally denoted $\exists_f\colon{ℙ}(X){→}{ℙ}(Y),$ and $R_f$ is generally denoted $\forall_f\colon{ℙ}(X){→}{ℙ}(Y).$ 
$$
\xymatrix{{ℙ}(X)\ar@/^1.2pc/[rr]^{\exists_f}\ar@/_1.2pc/[rr]^{\forall_f}&&{ℙ}(Y)\ar[ll]_{f^{-1}}.}
$$
We will see in the next example why this notation is apt.
\end{blockENG}

\begin{blockRUS}
\end{blockRUS}

\begin{exampleENG}
In logic or computer science, the quantifiers $\exists$ and $\forall$ are used to ask whether any or all elements of a set have a certain property. For example, one may have a set of natural numbers and want to know whether any or all are even or odd.
Let $Y=\{{\tt even,odd}\},$ and let $p\colon{ℕ}{→} Y$ be the function that takes assigns to each natural number its parity (even or odd). Because the elements of ${ℙ}({ℕ})$ and ${ℙ}(Y)$ are ordered by “inclusion of subsets”, we can construe these orders as categories (by Proposition~\ref{prop:preorders to cats}). That's all old; what's new is that we have adjunctions between these categories
$$
\xymatrix{{ℙ}({ℕ})\ar@/^1.2pc/[rr]^{\exists_p}\ar@/_1.2pc/[rr]^{\forall_p}&&{ℙ}(Y)\ar[ll]_{p^{-1}}.}
$$
Given a subset $U\subseteq{ℕ},$ i.e. an object $U\in\Ob({ℙ}({ℕ})),$ we investigate the objects $\exists_p(U),\forall_p(U).$ These are both subsets of $\{{\tt even,odd}\}.$ The set $\exists_p(U)$ includes the element {\tt even} if there exists an even number in $U$; it includes the element {\tt odd} if there exists an odd number in $U.$ Similarly, the set $\forall_p(U)$ includes the element {\tt even} if every even number is in $U$ and it includes {\tt odd} if every odd number is in $U.$
\footnote{It may not be clear that by this point we have also handled the question, “is every element of $U$ even?” One simply checks that {\tt odd} is not an element of $\exists_pU.$}

We explain just one of these in terms of the definitions. Let $V=\{{\tt even}\}\subseteq Y.$ Then $f^{-1}(V)\subseteq{ℕ}$ is the set of even numbers, and there is a morphism $f^{-1}(V){→} U$ in ${ℙ}({ℕ})$ if and only if $U$ contains all the even numbers. Therefore, the adjunction isomorphism $\Hom_{{ℙ}({ℕ})}(f^{-1}(V),U)\iso\Hom_{{ℙ}(Y)}(V,\forall_pU)$ says that $V\subseteq\forall_pU,$ i.e. $\forall_p(U)$ includes the element {\tt even} if and only if $U$ contains all the even numbers, as we said above.
\end{exampleENG}

\begin{exampleRUS}
\end{exampleRUS}

\begin{exerciseENG}
The national Scout jamboree is a gathering of Boy Scouts from troops across the US. Let $X$ be the set of Boy Scouts in the US, and let $Y$ be the set of Boy Scout troops in the US. Let $t\colon X{→} Y$ be the function that assigns to each Boy Scout his troop. Let $U\subseteq X$ be the set of Boy Scouts in attendance at this years jamboree. What is the meaning of the objects $\exists_tU$ and $\forall_tU?$
\end{exerciseENG}

\begin{exerciseRUS}
\end{exerciseRUS}

\begin{exerciseENG}
Let $X$ be a set and $U\subseteq X$ a subset. Find a set $Y$ and a function $f\colon X{→} Y$ such that $\exists_f(U)$ somehow tells you whether $U$ is non-empty, and such that $\forall_f(U)$ somehow tells you whether $U=X.$
\end{exerciseENG}

\begin{exerciseRUS}
\end{exerciseRUS}

\begin{blockENG}
In fact, “quantifiers as adjoints” is part of a larger story. Suppose we think of elements of a set $X$ as bins, or storage areas. An element of ${ℙ}(X)$ can be construed as an injection $U{↪} X,$ i.e. an assignment of a bin to each element of $U,$ with at most one element of $U$ in each bin. Relaxing that restriction, we may consider arbitrary sets $U$ and assignments $U{→} X$ of a bin to each element $u\in U.$ Given a function $f\colon X{→} Y,$ we can generalize $\exists_f$ and $\forall_f$ to functors denoted ${Σ}_f$ and ${Π}_f,$ which will parameterize disjoint unions and products (respectively) over $y\in Y.$ This will be discussed in Section~\ref{sec:data migration}.
\end{blockENG}

\begin{blockRUS}
\end{blockRUS}

%%%% Subsection %%%%

\subsection{\caseENGRUS{Universal concepts in terms of adjoints}{ / }{Универсальные конструкции в терминах сопряжений}}\label{sec:universal concepts}

\begin{blockENG}
In this section we discuss how universal concepts, i.e. initial objects and terminal objects, colimits and limits, are easily phrased in the language of adjoint functors. We will say that a functor $F\colon{𝓒}{→}{𝓓}$ {\em is a left adjoint} if there exists a functor $G\colon{𝓓}{→}{𝓒}$ such that $F$ is a left adjoint of $G.$ We showed in Proposition~\ref{prop:unicity of adjoints} that if $F$ is a left adjoint of some functor $G,$ then it is isomorphic to every other left adjoint of $G,$ and $G$ is isomorphic to every other right adjoint of $F.$
\end{blockENG}

\begin{blockRUS}
\end{blockRUS}

\begin{exampleENG}
Let ${𝓒}$ be a category and $t\colon{𝓒}{→}\underline{1}$ the unique functor to the terminal category. Then $t$ is a left adjoint if and only if ${𝓒}$ has a terminal object, and $t$ is a right adjoint if and only if ${𝓒}$ has an initial object. The proofs are dual, so let's focus on the first.

The functor $t$ has a right adjoint $R\colon\underline{1}{→}{𝓒}$ if and only if there is an isomorphism $$\Hom_{𝓒}(c,r)\iso\Hom_{\underline{1}}(t(c),1),$$ where $r=R(1).$ But $\Hom_{\underline{1}}(t(c),1)$ has one element. Thus $t$ has a right adjoint iff there is a unique morphism $c{→} r$ in ${𝓒}.$ This is the definition of $r$ being a terminal object.
\end{exampleENG}

\begin{exampleRUS}
\end{exampleRUS}

\begin{blockENG}
When we defined colimits and limits in Definitions~\ref{def:coslice and colimit} and~\ref{def:slice and limit} we did so for individual $I$-shaped diagrams $X\colon I{→}{𝓒}.$ Using adjoints we can define the limit of every $I$-shaped diagram in ${𝓒}$ at once.
\end{blockENG}

\begin{blockRUS}
\end{blockRUS}

\begin{blockENG}
Let $t\colon{𝓒}{→}\underline{1}$ denote the unique functor to the terminal category. Given an object $c\in\Ob({𝓒}),$ consider it as a functor $c\colon\underline{1}{→}{𝓒}.$ Then $c\circ t\colon I{→}{𝓒}$ is the {\em constant functor at $c$}\index{functor!constant}, sending each object in $I$ to the same ${𝓒}$-object $c,$ and every morphism in $I$ to $\id_c.$ This induces a functor that we denote by ${Δ}_t\colon{𝓒}{→}\Fun(I,{𝓒}).$
\end{blockENG}

\begin{blockRUS}
\end{blockRUS}

\begin{blockENG}
Suppose we want to take the colimit or limit of $X.$ We are given an object $X$ of $\Fun(I,{𝓒})$ and we want back an object of ${𝓒}.$ We could hope, and it turns out to be true, that the adjoints of ${Δ}_t$ are the limit and colimit. Indeed let ${Σ}_t\colon\Fun(I,{𝓒}){→}{𝓒}$ be the left adjoint of ${Δ}_t,$ and let ${Π}_t\colon\Fun(I,{𝓒}){→}{𝓒}$ be the right adjoint of ${Δ}_t.$ Then ${Σ}_t$ is the functor that takes colimits, and ${Π}_t$ is the functor that takes limits.
\end{blockENG}

\begin{blockRUS}
\end{blockRUS}

\begin{blockENG}
We will work with a generalization of colimits and limits in Section~\ref{sec:data migration}. But for now, let's bring this down to earth with a concrete example.
\end{blockENG}

\begin{blockRUS}
\end{blockRUS}

\begin{exampleENG}
Let ${𝓒}=\Set,$ and let $I=\underline{3}.$ The category $\Fun(I,\Set)$ is the category of $\{1,2,3\}$-indexed sets, e.g. $({ℤ},{ℕ},{ℤ})\in\Ob(\Fun(I,\Set))$ is an object of it. The functor ${Δ}_t\colon\Set{→}\Fun(I,\Set)$ acts as follows. Given a set $c\in\Ob(\Set),$ consider it as a functor $c\colon\underline{1}{→}\Set,$ and the composite $c\circ t\colon I{→}\Set$ is the constant functor. That is, ${Δ}_t(c)\colon I{→}\Set$ is the $\{1,2,3\}$--indexed set $(c,c,c).$

To say that ${Δ}_t$ has a right adjoint called ${Π}_t\colon\Fun(I,\Set){→}\Set$ and that it “takes limits” should mean that if we look through the definition of right adjoint, we will see that the formula will somehow yield the appropriate limit. Fix a functor $D\colon I{→}\Set,$ so $D(1),D(2),$ and $D(3)$ are sets. The limit $\lim D$ of $D$ is the product $D(1)\times D(2)\times D(3).$ For example, if $D=({ℤ},{ℕ},{ℤ})$ then $\lim D={ℤ}\times{ℕ}\times{ℤ}.$ How does this fact arise in the definition of adjoint?

The definition of ${Π}_t$ being the right adjoint to ${Δ}_t$ says that there is a natural isomorphism of sets, 
\begin{align}\label{dia:limit as adjoint}
\Hom_{\Fun(I,\Set)}({Δ}_t(c),D)\iso\Hom_\Set(c,{Π}_t(D)).
\end{align}
The left-hand side has elements $f\in\Hom_{\Fun(I,\Set)}({Δ}_t(c),D)$ that look like the left below, but having these three maps is equivalent to having the diagram to the right below:
$$\xymatrix{
c\ar[dd]^{f(1)}&c\ar[dd]^{f(2)}&c\ar[dd]^{f(3)}\\\\
D(1)&D(2)&D(3)
}
\hspace{1in}
\xymatrix{
&c\ar[ddl]_{f(1)}\ar[dd]^{f(2)}\ar[ddr]^{f(3)}\\\\
D(1)&D(2)&D(3)
}$$
The isomorphism in (\ref{dia:limit as adjoint}) says that choosing the three maps $f(1),f(2),f(3)$ is the same thing as choosing a function $c{→}{Π}_t(D).$ But this is very close to the universal property of limits: there is a unique map $\ell\colon c{→} D(1)\times D(2)\times D(3),$ so this product serves well as ${Π}_t$ as we have said. We're not giving a formal proof here, but what is missing at this point is the fact that certain diagrams have to commute. This comes down to the naturality of the isomorphism (\ref{dia:limit as adjoint}). The map $\ell$ induces a naturality square
$$\xymatrix{{Δ}_t(c)\ar[r]^{{Δ}_t(\ell)}\ar[d]_f&{Δ}_t{Π}_tD\ar[d]^\pi\\D\ar@{=}[r]&D}$$
which says that the following diagram commutes:
$$\xymatrix{&c\ar[ddl]_{f(1)}\ar[dd]^{f(2)}\ar[ddr]^{f(3)}\ar@/_2pc/[dddd]_\ell\\\\
D(1)&D(2)&D(3)\\\\
&D(1)\times D(2)\times D(3)\ar[uul]^{\pi_1}\ar[uu]_{\pi_2}\ar[uur]_{\pi_3}
}$$
\end{exampleENG}

\begin{exampleRUS}
\end{exampleRUS}

\begin{blockENG}
It is not hard to show that the composition of left adjoints is a left adjoint, and the composition of right adjoints is a right adjoint. In the following example we show how currying (as in Sections~\ref{sec:currying} and~\ref{ex:other adjunctions}) arises out of a certain combination of data migration functors. 
\end{blockENG}

\begin{blockRUS}
\end{blockRUS}

\begin{exampleENG}[Currying via ${Δ},{Σ},{Π}$]\index{currying!via data migration functors}
Let $A,B,$ and $C$ be sets. Consider the unique functor $a\colon A{→}\underline{1}$ and consider $B$ and $C$ as functors $\underline{1}\Too{B}\Set$ and $\underline{1}\Too{C}\Set$ respectively. 
$$\xymatrix{A\ar[r]^a&\underline{1}\ar@/^1pc/[r]^B\ar@/_1pc/[r]_C&\Set}$$
Note that $\underline{1}\set\iso\Set,$ and we will elide the difference. Our goal is to see currying arise out of the adjunction between ${Σ}_a\circ{Δ}_a$ and ${Π}_a\circ{Δ}_a,$ which tells us that there is an isomorphism
\begin{align}\label{dia:migration for currying}
\Hom_\Set({Σ}_a{Δ}_a(B),C)\iso\Hom_\Set(B,{Π}_a{Δ}_a(C)).
\end{align}

By definition, ${Δ}_a(B)\colon A{→}\Set$ assigns to each element $a\in A$ the set $B.$ Since ${Σ}_A$ takes disjoint unions, we have a bijection
$${Σ}_a({Δ}_a(B))=\left(\coprod_{a\in A}B\right)\iso A\times B.$$ 
Similarly ${Δ}_a(C)\colon A{→}\Set$ assigns to each element $a\in A$ the set $C.$ Since ${Π}_A$ takes products, we have a bijection
$${Π}_a({Δ}_a(C))=\left(\prod_{a\in A}C\right)\iso C^A.$$
The currying isomorphism $\Hom_\Set(A\times B,C)\iso\Hom_\Set(B,C^A)$ falls out of (\ref{dia:migration for currying}).
\end{exampleENG}

\begin{exampleRUS}[Currying via ${Δ},{Σ},{Π}$]\index{currying!via data migration functors}
\end{exampleRUS}

%%%% Subsection %%%%

\subsection{\caseENGRUS{Preservation of colimits or limits}{ / }{Сохранение копределов или пределов}}

\begin{blockENG}
One useful fact about adjunctions is that left adjoints preserve all colimits and right adjoints preserve all limits. 
\end{blockENG}

\begin{blockRUS}
\end{blockRUS}

\begin{propositionENG}
Let $\adjoint{L}{{𝓑}}{{𝓐}}{R}$ be an adjunction. For any indexing category $I$ and functor $D\colon I{→}{𝓑},$ if $D$ has a colimit in ${𝓑}$ then there is a unique isomorphism 
$$L(\colim D)\iso \colim (L\circ D).$$

Similarly, for any $I\in\Ob(\Cat)$ and functor $D\colon I{→}{𝓐},$ if $D$ has a limit in ${𝓐}$ then there is a unique isomorphism 
$$R(\lim D)\iso \lim (R\circ D).$$
\end{propositionENG}

\begin{propositionRUS}
\end{propositionRUS}

\begin{proofENG}
The proof is simple if one knows the Yoneda lemma (Section~\ref{sec:yoneda}). I have decided to skip it to keep the book shorter. See \cite{Mac}.
\end{proofENG}

\begin{proofRUS}
\end{proofRUS}

\begin{exampleENG}
Since $\Ob\colon\Cat{→}\Set$ is both a left adjoint and a right adjoint, it must preserve both limits and colimits. This means that if you want to know the set of objects in the fiber product of some categories, you can simply take the fiber product of the set of objects in those categories, $$\Ob({𝓐}\times_{𝓒}{𝓑})\iso\Ob({𝓐})\times_{\Ob({𝓒})}\Ob({𝓑}).$$ While the right-hand side might look daunting, it is just a fiber product in $\Set$ which is quite understandable.

This is greatly simplifying. If one thinks through what defines a limit in $\Cat,$ one is dragged through notions of slice categories and terminal objects in them. These slice categories are in $\Cat$ so they involve several categories and functors, and it gets hairy or even hopeless to a beginner. Knowing that the objects are given by a simple fiber product makes the search for limits in $\Cat$ much simpler. 

For example, if $[n]$ is the linear order category of length $n$ then $[n]\times[m]$ has $nm+n+m+1$ objects because $[n]$ has $n+1$ objects and $[m]$ has $m+1$ objects. 
\end{exampleENG}

\begin{exampleRUS}
\end{exampleRUS}

\begin{exampleENG}
The “path poset” functor $L\colon\Grph{→}\PrO$ given by existence of paths (see Exercise~\ref{exc:grph to pro}) is left adjoint to the functor $R\colon\PrO{→}\Grph$ given by replacing $\leq$'s by arrows. This means that $L$ preserves colimits. So taking the union of graphs $G$ and $H$ results in a graph whose path poset  $L(G\sqcup H)$ is the union of the path posets of $G$ and $H.$ But this is not so for products. 

Let $G=H=\fbox{\xymatrix{\LMO{a}\ar[r]^f&\LMO{b}}}.$ Then $L(G)=L(H)=[1],$ the linear order of length 1. But the product $G\times H$ in $\Grph$ looks like the graph 
$$\xymatrix{\LMO{(a,a)}\ar[rd]&\LMO{(a,b)}\\\LMO{(b,a)}&\LMO{(b,b)}}$$
Its preorder $L(G\times H)$ does not have $(a,a)\leq(a,b),$ whereas this is the case in $L(G)\times L(H).$
\end{exampleENG}

\begin{exampleRUS}
\end{exampleRUS}

%%%% Subsection %%%%

\subsection{\caseENGRUS{Data migration}{ / }{Миграция баз данных}}\label{sec:data migration}

\begin{blockENG}
As we saw in Sections~\ref{sec:schemas and cats intro} and~\ref{sec:instances}, a database schema is a category ${𝓒}$ and an instance is a functor $I\colon{𝓒}{→}\Set.$  
\end{blockENG}

\begin{blockRUS}
\end{blockRUS}

\begin{notationENG}
Let ${𝓒}$ be a category. Throughout this section we denote by ${𝓒}\set$ the category $\Fun({𝓒},\Set)$ of functors from ${𝓒}$ to $\Set,$ i.e. the category of instances on ${𝓒}.$ 
\end{notationENG}

\begin{notationRUS}
\end{notationRUS}

\begin{blockENG}
In this section we discuss what happens to the resulting instances when different schemas are connected by a functor, say $F\colon{𝓒}{→}{𝓓}.$\index{data migration} It turns out that three adjoint functors emerge: ${Δ}_F\colon{𝓓}\set{→}{𝓒}\set,$ ${Σ}_F\colon{𝓒}\set{→}{𝓓}\set,$ and ${Π}_F\colon{𝓒}\set{→}{𝓓}\set,$ where ${Δ}_F$ is adjoint to both, 
$$
\adjoint{{Σ}_F}{{𝓒}\set}{{𝓓}\set}{{Δ}_F}
\hspace{.7in}
\adjoint{{Δ}_F}{{𝓓}\set}{{𝓒}\set}{{Π}_F.}
$$
It turns out that almost all the basic database operations are captured by these three functors. For example, ${Δ}_F$ handles the job of duplicating or deleting tables, as well as duplicating or deleting columns in a single table. The functor ${Σ}_F$ handles taking unions, and the functor ${Π}_F$ handles joining tables together, matching columns, or selecting the rows with certain properties (e.g. everyone whose first name is Mary).
\end{blockENG}

\begin{blockRUS}
\end{blockRUS}

%% Subsubsection %%

\subsubsection{\caseENGRUS{Pullback}{ / }{Пулбек}: \texorpdfstring{${Δ}$}{Δ}}

\begin{blockENG}
Given a functor $F\colon{𝓒}{→}{𝓓}$ and a functor $I\colon{𝓓}{→}\Set,$ we can compose them to get a functor $I\circ F\colon{𝓒}{→}\Set.$ In other words, the presence of $F$ provides a way to convert ${𝓓}$-instances into ${𝓒}$-instances. In fact this conversion is functorial, meaning that morphisms of ${𝓓}$-instances are sent to morphisms of ${𝓒}$-instances. We denote the resulting functor by ${Δ}_F\colon{𝓓}\set{→}{𝓒}\set$ and call it {\em pullback along $F$}.
\end{blockENG}

\begin{blockRUS}
\end{blockRUS}

\begin{blockENG}
We have seen an example of this before in Example~\ref{ex:whiskering}, where we showed how a monoid homomorphism $F\colon{𝓜}'{→}{𝓜}$ could add functionality to a finite state machine. More generally, we can use pullbacks to reorganize data, copying and deleting tables and columns. 
\end{blockENG}

\begin{blockRUS}
\end{blockRUS}

\begin{remarkENG}
Given a functor $F\colon{𝓒}{→}{𝓓},$ which we think of as a schema translation, the functor ${Δ}_F\colon{𝓓}\set{→}{𝓒}\set$ “goes the opposite way”.\index{data migration!pullback ${Δ}$} The reasoning is simple to any explain (composition of functors) but something about it is often very strange to people, at first. The rough idea of this “contravariance” is captured by the role-reversal in the following slogan:
\end{remarkENG}

\begin{remarkRUS}
\end{remarkRUS}

\begin{sloganENG} 
If I get my information from you, then your information becomes my information. 
\end{sloganENG}

\begin{sloganRUS} 
\end{sloganRUS}

\begin{blockENG}
Consider the following functor $F\colon{𝓒}{→}{𝓓}$: 
\footnote{This example was taken from \cite{Sp1}, \url{http://arxiv.org/abs/1009.1166}.}
\begin{align}\label{dia:translation}
{𝓒}{\coloneqq}\parbox{1.2in}{\fbox{\xymatrix@=10pt{&\LTO{SSN}\\&\LTO{First}\\\color{red}{\LTO{T1}}\ar[uur]\ar[ur]\ar[dr]&&\color{red}{\LTO{T2}}\ar[ul]\ar[dl]\ar[ddl]\\&\LTO{Last}\\&\LTO{Salary}}}}\Too{F}\parbox{.8in}{\fbox{\xymatrix@=10pt{&\LTO{SSN}\\&\LTO{First}\\\color{red}{\LTO{T}}\ar[uur]\ar[ur]\ar[dr]\ar[ddr]\\&\LTO{Last}\\&\LTO{Salary}}}}=:{𝓓}
\end{align}
\end{blockENG}

\begin{blockRUS}
\end{blockRUS}

\begin{blockENG}
Let's spend a moment recalling how to “read” schemas. In schema ${𝓒}$ there are leaf tables {\tt SSN, First, Last, Salary}, which represent different kinds of basic data. More interestingly, there are two {\em fact tables}\index{schema!leaf table}\index{schema!fact table}. The first is called {\tt T1} and it relates {\tt SSN, First,} and ${\tt Last}.$ The second is called {\tt T2} and it relates {\tt First, Last,} and {\tt Salary}.
\end{blockENG}

\begin{blockRUS}
\end{blockRUS}

\begin{blockENG}
The functor $F\colon{𝓒}{→}{𝓓}$ relates ${𝓒}$ to a schema with a single fact table relating all four attributes: {\tt SSN, First, Last,} and {\tt Salary}. We are interested in ${Δ}_F\colon{𝓓}\set{→}{𝓒}\set.$ Suppose given the following database instance $I\colon{𝓓}{→}\Set$ on ${𝓓}$:
$$
\begin{tabular}{| l || l | l | l | l |}\bhline\multicolumn{5}{| c |}{{\tt T}}\\\bhline {\bf ID}&{\bf SSN}&{\bf First}&{\bf Last}&{\bf Salary}\\\bbhline XF667&115-234&Bob&Smith&\$250\\\hline XF891&122-988&Sue&Smith&\$300\\\hline XF221&198-877&Alice&Jones&\$100\\\bhline
\end{tabular}
$$
$$
\begin{tabular}{| l ||}\bhline\multicolumn{1}{| c |}{{\tt SSN}}\\\bhline {\bf ID}\\\bbhline 115-234\\\hline 118-334\\\hline 122-988\\\hline 198-877\\\hline 342-164\\\bhline
\end{tabular}\hspace{.5in}
\begin{tabular}{| l ||}\bhline\multicolumn{1}{| c |}{{\tt First}}\\\bhline {\bf ID}\\\bbhline Adam\\\hline Alice\\\hline Bob\\\hline Carl\\\hline Sam\\\hline Sue\\\bhline
\end{tabular}
\hspace{.5in}
\begin{tabular}{| l ||}\bhline\multicolumn{1}{| c |}{{\tt Last}}\\\bhline {\bf ID}\\\bbhline Jones\\\hline Miller\\\hline Pratt\\\hline Richards\\\hline Smith\\\bhline
\end{tabular}
\hspace{.5in}
\begin{tabular}{| l ||}\bhline\multicolumn{1}{| c |}{{\tt Salary}}\\\bhline {\bf ID}\\\bbhline \$100\\\hline \$150\\\hline \$200\\\hline \$250\\\hline \$300\\\bhline
\end{tabular}
$$
\end{blockENG}

\begin{blockRUS}
\end{blockRUS}

\begin{blockENG}
How do you get the instance ${Δ}_F(I)\colon{𝓒}{→}\Set?$ The formula was given above: compose $I$ with $F.$ In terms of tables, it feels like duplicating table {\tt T} as {\tt T1} and {\tt T2}, but deleting a column from each in accordance with the definition of ${𝓒}$ in (\ref{dia:translation}). Here is the result, ${Δ}_F(I),$ in table form:
\end{blockENG}

\begin{blockRUS}
\end{blockRUS}

\begin{blockENG}
$$\begin{tabular}{| l || l | l | l |}\bhline\multicolumn{4}{| c |}{{\tt T1}}\\\bhline {\bf ID}&{\bf SSN}&{\bf First}&{\bf Last}\\\bbhline XF667&115-234&Bob&Smith\\\hline XF891&122-988&Sue&Smith\\\hline XF221&198-877&Alice&Jones\\\bhline
\end{tabular}
\hspace{.5in}
\begin{tabular}{| l || l | l | l |}\bhline\multicolumn{4}{| c |}{{\tt T2}}\\\bhline {\bf ID}&{\bf First}&{\bf Last}&{\bf Salary}\\\bbhline XF221&Alice&Jones&\$100\\\hline XF667&Bob&Smith&\$250\\\hline XF891&Sue&Smith&\$300\\\hline 
\end{tabular}
$$
$$
\begin{tabular}{| l ||}\bhline\multicolumn{1}{| c |}{{\tt SSN}}\\\bhline {\bf ID}\\\bbhline 115-234\\\hline 118-334\\\hline 122-988\\\hline 198-877\\\hline 342-164\\\bhline
\end{tabular}\hspace{.5in}
\begin{tabular}{| l ||}\bhline\multicolumn{1}{| c |}{{\tt First}}\\\bhline {\bf ID}\\\bbhline Adam\\\hline Alice\\\hline Bob\\\hline Carl\\\hline Sam\\\hline Sue\\\bhline
\end{tabular}
\hspace{.5in}
\begin{tabular}{| l ||}\bhline\multicolumn{1}{| c |}{{\tt Last}}\\\bhline {\bf ID}\\\bbhline Jones\\\hline Miller\\\hline Pratt\\\hline Richards\\\hline Smith\\\bhline
\end{tabular}
\hspace{.5in}
\begin{tabular}{| l ||}\bhline\multicolumn{1}{| c |}{{\tt Salary}}\\\bhline {\bf ID}\\\bbhline \$100\\\hline \$150\\\hline \$200\\\hline \$250\\\hline \$300\\\bhline
\end{tabular}
$$
\end{blockENG}

\begin{blockRUS}
\end{blockRUS}

\begin{exerciseENG}\index{leaf table}
Let ${𝓒}=(G,\simeq)$ be a schema. A leaf table is an object $c\in\Ob({𝓒})$ with no outgoing arrows.
\sexc Write the condition of being a “leaf table” mathematically in three different languages: that of graphs (using symbols $V,A,src,tgt$), that of categories (using $\Hom_{𝓒},$ etc.), and that of tables (in terms of columns, tables, rows, etc.).
\item In the language of categories, is there a difference between a terminal object and a leaf table? Explain.
\endsexc
\end{exerciseENG}

\begin{exerciseRUS}\index{leaf table}
\end{exerciseRUS}

\begin{exerciseENG}
Consider the schemas $$[1]=\fbox{$\LMO{\color{blue}{0}}\Too{f}\LMO{\color{red}{1}}$}\hsp\tn{and}\hsp [2]=\fbox{$\LMO{\color{blue}{0}}\Too{g}\LMO{1}\Too{h}\LMO{\color{red}{2}}$},$$ and the functor $F\colon [1]{→}[2]$ given by sending $0\mapsto 0$ and $1\mapsto 2.$ 
\sexc How many possibilities are there for $F(f)?$
\item Now suppose $I\colon[2]{→}\Set$ is given by the following tables. 
$$
\begin{tabular}{| l || l |}
\bhline
\multicolumn{2}{|c|}{0}\\\bhline
{\bf ID}&{\bf g}\\\bbhline
Am&To be verb\\\hline
Baltimore&Place\\\hline
Carla&Person\\\hline
Develop&Action verb\\\hline
Edward&Person\\\hline
Foolish&Adjective\\\hline
Green&Adjective\\\bhline
\end{tabular}
\hspace{.4in}
\begin{tabular}{| l || l |}
\bhline
\multicolumn{2}{|c|}{1}\\\bhline
{\bf ID}&{\bf h}\\\bbhline
Action verb&Verb\\\hline
Adjective&Adjective\\\hline
Place&Noun\\\hline
Person&Noun\\\hline
To be verb&Verb\\\bhline
\end{tabular}
\hspace{.4in}
\begin{tabular}{| l ||}
\bhline
\multicolumn{1}{| c |}{2}\\\bhline
{\bf ID}\\\bbhline
Adjective\\\hline
Noun\\\hline
Verb\\\bhline
\end{tabular}
$$
Write out the two tables associated to the $[1]$-instance ${Δ}_F(I)\colon[1]{→}\Set.$
\endsexc
\end{exerciseENG}

\begin{exerciseRUS}
\end{exerciseRUS}

%% Subsubsection %%

\subsubsection{\caseENGRUS{Left pushforward}{ / }{Левое расширение Кана}: \texorpdfstring{${Σ}$}{Σ}}\label{sec:left push}

\begin{blockENG}
Let $F\colon{𝓒}{→}{𝓓}$ be a functor. The functor ${Δ}_F\colon{𝓓}\set{→}{𝓒}\set$ has a left adjoint, ${Σ}_F\colon{𝓒}\set{→}{𝓓}\set.$ The rough idea is that ${Σ}_F$ performs parameterized colimits. Given an instance $I\colon{𝓒}{→}\Set,$ we get an instance on ${𝓓}$ that acts as follows. For each object $d\in\Ob({𝓓}),$ the set ${Σ}_F(I)(d)$ is the colimit (think, union) of some diagram back home in ${𝓒}.$ 
\end{blockENG}

\begin{blockRUS}
\end{blockRUS}

\begin{blockENG}
Left pushforwards (also known as left Kan extensions) are discussed at length in \cite{Sp1}; here we begin with some examples from that paper.\index{Kan extension!left}
\end{blockENG}

\begin{blockRUS}
\end{blockRUS}

\begin{exampleENG}\label{ex:left pushforward and skolem}
We again use the functor $F\colon{𝓒}{→}{𝓓}$ drawn below\index{data migration!left pushforward ${Σ}$}
\begin{align}\tag{\ref{dia:translation}}{𝓒}{\coloneqq}\parbox{1.2in}{\fbox{\xymatrix@=10pt{&\LTO{SSN}\\&\LTO{First}\\\color{red}{\LTO{T1}}\ar[uur]\ar[ur]\ar[dr]&&\color{red}{\LTO{T2}}\ar[ul]\ar[dl]\ar[ddl]\\&\LTO{Last}\\&\LTO{Salary}}}}\Too{F}\parbox{.8in}{\fbox{\xymatrix@=10pt{&\LTO{SSN}\\&\LTO{First}\\\color{red}{\LTO{T}}\ar[uur]\ar[ur]\ar[dr]\ar[ddr]\\&\LTO{Last}\\&\LTO{Salary}}}}=:{𝓓}
\end{align}
We will be applying the left pushforward ${Σ}_F\colon{𝓒}\set{→}{𝓓}\set$ to the following instance $I\colon{𝓒}{→}\Set$: 
$$
\begin{tabular}{| l || l | l | l |}\bhline\multicolumn{4}{| c |}{{\tt T1}}\\\bhline {\bf ID}&{\bf SSN}&{\bf First}&{\bf Last}\\\bbhline T1-001&115-234&Bob&Smith\\\hline T1-002&122-988&Sue&Smith\\\hline T1-003&198-877&Alice&Jones\\\bhline
\end{tabular}
\hsp
\begin{tabular}{| l || l | l | l |}\bhline\multicolumn{4}{| c |}{{\tt T2}}\\\bhline {\bf ID}&{\bf First}&{\bf Last}&{\bf Salary}\\\bbhline T2-001&Alice&Jones&\$100\\\hline T2-002&Sam&Miller&\$150\\\hline T2-004&Sue&Smith&\$300\\\hline T2-010&Carl&Pratt&\$200 \\\bhline
\end{tabular}
$$
$$
\begin{tabular}{| l ||}\bhline\multicolumn{1}{| c |}{{\tt SSN}}\\\bhline {\bf ID}\\\bbhline 115-234\\\hline 118-334\\\hline 122-988\\\hline 198-877\\\hline 342-164\\\bhline
\end{tabular}
\hsp\hsp
\begin{tabular}{| l ||}\bhline\multicolumn{1}{| c |}{{\tt First}}\\\bhline {\bf ID}\\\bbhline Adam\\\hline Alice\\\hline Bob\\\hline Carl\\\hline Sam\\\hline Sue\\\bhline
\end{tabular}
\hsp\hsp
\begin{tabular}{| l ||}\bhline\multicolumn{1}{| c |}{{\tt Last}}\\\bhline {\bf ID}\\\bbhline Jones\\\hline Miller\\\hline Pratt\\\hline Richards\\\hline Smith\\\bhline
\end{tabular}
\hsp\hsp
\begin{tabular}{| l ||}\bhline\multicolumn{1}{| c |}{{\tt Salary}}\\\bhline {\bf ID}\\\bbhline \$100\\\hline \$150\\\hline \$200\\\hline \$250\\\hline \$300\\\bhline
\end{tabular}
$$

The functor $F\colon{𝓒}{→}{𝓓}$ sent both tables {\tt T1} and {\tt T2} to table {\tt T}. Applying ${Σ}_F$ will take the what was in {\tt T1} and {\tt T2} and put the union in {\tt T}. The result ${Σ}_FI\colon{𝓓}{→}\Set$ is as follows:
\begin{center}
\begin{tabular}{| l || l | l | l | l |}\bhline\multicolumn{5}{| c |}{{\tt T}}\\\bhline {\bf ID}&{\bf SSN}&{\bf First}&{\bf Last}&{\bf Salary}\\\bbhline  T1-001&115-234&Bob&Smith&T1-001.Salary\\\hline T1-002&122-988&Sue&Smith&T1-002.Salary\\\hline T1-003&198-877&Alice&Jones&T1-003.Salary\\\hline T2-001&T2-A101.SSN&Alice&Jones&\$100\\\hline T2-002&T2-A102.SSN&Sam&Miller&\$150\\\hline T2-004&T2-004.SSN&Sue&Smith&\$300\\\hline T2-010&T2-A110.SSN&Carl&Pratt&\$200 \\\bhline
\end{tabular}
\end{center}
$$
\begin{tabular}{| l ||}\bhline
\multicolumn{1}{| c |}{{\tt SSN}}\\\bhline 
{\bf ID}\\\bbhline 
115-234\\\hline 
118-334\\\hline 
122-988\\\hline 
198-877\\\hline 
342-164\\\hline
T2-001.SSN\\\hline
T2-002.SSN\\\hline
T2-004.SSN\\\hline
T2-010.SSN\\\bhline
\end{tabular}
\hspace{.5in}
\begin{tabular}{| l ||}\bhline
\multicolumn{1}{| c |}{{\tt First}}\\\bhline 
{\bf ID}\\\bbhline 
Adam\\\hline 
Alice\\\hline 
Bob\\\hline 
Carl\\\hline 
Sam\\\hline 
Sue\\\bhline
\end{tabular}
\hspace{.5in}
\begin{tabular}{| l ||}\bhline
\multicolumn{1}{| c |}{{\tt Last}}\\\bhline 
{\bf ID}\\\bbhline 
Jones\\\hline 
Miller\\\hline 
Pratt\\\hline 
Richards\\\hline 
Smith\\\bhline
\end{tabular}
\hspace{.5in}
\begin{tabular}{| l ||}\bhline
\multicolumn{1}{| c |}{{\tt Salary}}\\\bhline 
{\bf ID}\\\bbhline 
\$100\\\hline 
\$150\\\hline 
\$200\\\hline 
\$250\\\hline 
\$300\\\hline
T1-001.Salary\\\hline
T1-002-Salary\\\hline
T1-003-Salary\\\bhline
\end{tabular}
$$

As you can see, there was no set salary information for any data coming from table {\tt T1} nor any set SSN information for any data coming form table {\tt T2}. But the definition of adjoint, given in Definition~\ref{def:adjunction}, yielded the universal response: freely add new variables that take the place of missing information. It turns out that this idea already has a name in logic, {\em Skolem variables}, and a name in database theory, {\em labeled nulls}.\index{labeled null}\index{Skolem variable}
\end{exampleENG}

\begin{exampleRUS}\label{ex:left pushforward and skolem}
\end{exampleRUS}

\begin{exerciseENG}
Consider the functor $F\colon\underline{3}{→}\underline{2}$ sending $1\mapsto 1, 2\mapsto 2, 3\mapsto 2.$
\sexc Write down an instance $I\colon\underline{3}{→}\Set.$
\item Given the description that “${Σ}_F$ performs a parameterized colimit”, make an educated guess about what ${Σ}_F(I)$ will be. Give your answer in the form of two sets that are made up from the three sets you already wrote down.
\endsexc
\end{exerciseENG}

\begin{exerciseRUS}
\end{exerciseRUS}

\begin{blockENG}
We now briefly give the actual formula for computing left pushforwards. Suppose that $F\colon{𝓒}{→}{𝓓}$ is a functor and let $I\colon{𝓒}{→}\Set$ be a set-valued functor on ${𝓒}.$ Then ${Σ}_F(I)\colon{𝓓}{→}\Set$ is defined as follows. Given an object $d\in\Ob({𝓓})$ we first form the comma category (see Definition~\ref{def:comma category}) for the setup
$${𝓒}\To{F}{𝓓}\From{d}\underline{1}$$
and denote it by $(F{↓} d).$ There is a canonical projection functor $\pi\colon(F{↓} d){→}{𝓒},$ which we can compose with $I\colon{𝓒}{→}\Set$ to obtain a functor $(F{↓} d){→}\Set.$ We are ready to define ${Σ}_F(I)(d)$ to be its colimit,
$${Σ}_F(I)(d){\coloneqq}\colim_{(F{↓} d)}I\circ\pi.$$
We have defined ${Σ}_F(I)\colon{𝓓}{→}\Set$ on objects $d\in\Ob({𝓓}).$ As for morphisms we will be even more brief, but one can see \cite{Sp1} for details. Given a morphism $g\colon d{→} d'$ one notes that there is an induced functor $(F{↓} g)\colon (F{↓} d){→}(F{↓} d')$ and a commutative diagram of categories:
$$
\xymatrix@=15pt{
(F{↓} d)\ar[rr]^{(F{↓} g)}\ar[ddr]^{\pi}\ar[ddddr]_{I\circ\pi}&&(F{↓} d')\ar[ddl]_{\pi'}\ar[ddddl]^{I\circ\pi'}\\\\
&{𝓒}\ar[dd]^I\\\\
&\Set
}
$$
By the universal property of colimits, this induces the required function $$\colim_{(F{↓} d)}I\circ\pi\Too{{Σ}_F(I)(g)}\colim_{(F{↓} d')}I\circ\pi'.$$
\end{blockENG}

\begin{blockRUS}
\end{blockRUS}

%% Subsubsection %%

\subsubsection{\caseENGRUS{Right pushforward}{ / }{Правое расширение Кана}: \texorpdfstring{${Π}$}{Π}}

\begin{blockENG}
Let $F\colon{𝓒}{→}{𝓓}$ be a functor. We heard in Section~\ref{sec:left push} that the functor ${Δ}_F\colon{𝓓}\set{→}{𝓒}\set$ has a left adjoint. Here we explain that it has a right adjoint, ${Π}_F\colon{𝓒}\set{→}{𝓓}\set$ as well. The rough idea is that ${Π}_F$ performs parameterized limits. Given an instance $I\colon{𝓒}{→}\Set,$ we get an instance on ${𝓓}$ that acts as follows. For each object $d\in\Ob({𝓓}),$ the set ${Π}_F(I)(d)$ is the limit (think, fiber product) of some diagram back home in ${𝓒}.$ 
\end{blockENG}

\begin{blockRUS}
\end{blockRUS}

\begin{blockENG}
Right pushforwards (also known as right Kan extensions) are discussed at length in \cite{Sp1}\index{data migration!right pushforward ${Π}$}; here we begin with some examples from that paper.\index{Kan extension!right}
\end{blockENG}

\begin{blockRUS}
\end{blockRUS}

\begin{exampleENG}
We once again use the functor $F\colon{𝓒}{→}{𝓓}$ from Example~\ref{ex:left pushforward and skolem}. We will apply the right pushforward ${Π}_F$ to instance $I\colon{𝓒}{→}\Set$ from that example.
\footnote{To repeat for convenience,
\begin{align}\tag{\ref{dia:translation}}{𝓒}{\coloneqq}\parbox{1.2in}{\fbox{\xymatrix@=10pt{&\LTO{SSN}\\&\LTO{First}\\\color{red}{\LTO{T1}}\ar[uur]\ar[ur]\ar[dr]&&\color{red}{\LTO{T2}}\ar[ul]\ar[dl]\ar[ddl]\\&\LTO{Last}\\&\LTO{Salary}}}}\Too{F}\parbox{.8in}{\fbox{\xymatrix@=10pt{&\LTO{SSN}\\&\LTO{First}\\\color{red}{\LTO{T}}\ar[uur]\ar[ur]\ar[dr]\ar[ddr]\\&\LTO{Last}\\&\LTO{Salary}}}}=:{𝓓}
\end{align}
$I\colon{𝓒}{→}\Set$ is 
$$
\begin{tabular}{| l || l | l | l |}\bhline\multicolumn{4}{| c |}{{\tt T1}}\\\bhline {\bf ID}&{\bf SSN}&{\bf First}&{\bf Last}\\\bbhline T1-001&115-234&Bob&Smith\\\hline T1-002&122-988&Sue&Smith\\\hline T1-003&198-877&Alice&Jones\\\bhline
\end{tabular}
\hsp
\begin{tabular}{| l || l | l | l |}\bhline\multicolumn{4}{| c |}{{\tt T2}}\\\bhline {\bf ID}&{\bf First}&{\bf Last}&{\bf Salary}\\\bbhline T2-001&Alice&Jones&\$100\\\hline T2-002&Sam&Miller&\$150\\\hline T2-004&Sue&Smith&\$300\\\hline T2-010&Carl&Pratt&\$200 \\\bhline
\end{tabular}
$$
$$
\begin{tabular}{| l ||}\bhline\multicolumn{1}{| c |}{{\tt SSN}}\\\bhline {\bf ID}\\\bbhline 115-234\\\hline 118-334\\\hline 122-988\\\hline 198-877\\\hline 342-164\\\bhline
\end{tabular}
\hsp\hsp
\begin{tabular}{| l ||}\bhline\multicolumn{1}{| c |}{{\tt First}}\\\bhline {\bf ID}\\\bbhline Adam\\\hline Alice\\\hline Bob\\\hline Carl\\\hline Sam\\\hline Sue\\\bhline
\end{tabular}
\hsp\hsp
\begin{tabular}{| l ||}\bhline\multicolumn{1}{| c |}{{\tt Last}}\\\bhline {\bf ID}\\\bbhline Jones\\\hline Miller\\\hline Pratt\\\hline Richards\\\hline Smith\\\bhline
\end{tabular}
\hsp\hsp
\begin{tabular}{| l ||}\bhline\multicolumn{1}{| c |}{{\tt Salary}}\\\bhline {\bf ID}\\\bbhline \$100\\\hline \$150\\\hline \$200\\\hline \$250\\\hline \$300\\\bhline
\end{tabular}
$$
}

The instance ${Π}_F(I)$ will put data in all 5 tables in ${𝓓}.$ In {\tt T} it will put pairs $(t_1,t_2)$ where $t_1$ is a row in {\tt T1} and $t_2$ is a row in {\tt T2} for which the first and last names agree. It will copy the leaf tables exactly, so we do not display them here; the following is the table {\tt T} for ${Π}_F(I)$:
\begin{center}
\begin{tabular}{| l || l | l | l | l |}\bhline\multicolumn{5}{| c |}{{\tt T}}\\\bhline {\bf ID}&{\bf SSN}&{\bf First}&{\bf Last}&{\bf Salary}\\\bbhline  T1-002T2-A104&122-988&Sue&Smith&\$300\\\hline T1-003T2-A101&198-877&Alice&Jones&\$100\\\bhline
\end{tabular}
\end{center}
Looking at {\tt T1} and {\tt T2}, there were only two ways to match first and last names.
\end{exampleENG}

\begin{exampleRUS}
\end{exampleRUS}

\begin{exerciseENG}
Consider the functor $F\colon\underline{3}{→}\underline{2}$ sending $1\mapsto 1, 2\mapsto 2, 3\mapsto 2.$
\sexc Write down an instance $I\colon\underline{3}{→}\Set.$
\item Given the description that “${Π}_F$ performs a parameterized limit”, make an educated guess about what ${Π}_F(I)$ will be. Give your answer in the form of two sets that are made up from the three sets you already wrote down.
\endsexc
\end{exerciseENG}

\begin{exerciseRUS}
\end{exerciseRUS}

\begin{blockENG}
We now briefly give the actual formula for computing right pushforwards. Suppose that $F\colon{𝓒}{→}{𝓓}$ is a functor and let $I\colon{𝓒}{→}\Set$ be a set-valued functor on ${𝓒}.$ Then ${Π}_F(I)\colon{𝓓}{→}\Set$ is defined as follows. Given an object $d\in\Ob({𝓓})$ we first form the comma category (see Definition~\ref{def:comma category}) for the setup
$$\underline{1}\To{d}{𝓓}\From{F}{𝓒}$$
and denote it by $(d{↓} F).$ There is a canonical projection functor $\pi\colon(d{↓} F){→}{𝓒},$ which we can compose with $I\colon{𝓒}{→}\Set$ to obtain a functor $(d{↓} F){→}\Set.$ We are ready to define ${Π}_F(I)(d)$ to be its limit,
$${Π}_F(I)(d){\coloneqq}\lim_{(d{↓} F)}I\circ\pi.$$
We have defined ${Π}_F(I)\colon{𝓓}{→}\Set$ on objects $d\in\Ob({𝓓}).$ As for morphisms we will be even more brief, but one can see \cite{Sp1} for details. Given a morphism $g\colon d{→} d'$ one notes that there is an induced functor $(g{↓} F)\colon (d'{↓} F){→}(d{↓} F)$ and a commutative diagram of categories:
$$
\xymatrix@=15pt{
(d'{↓} F)\ar[rr]^{(g{↓} F)}\ar[ddr]^{\pi'}\ar[ddddr]_{I\circ\pi'}&&(d{↓} F)\ar[ddl]_{\pi}\ar[ddddl]^{I\circ\pi}\\\\
&{𝓒}\ar[dd]^I\\\\
&\Set
}
$$
By the universal property of limits, this induces the required function $$\lim_{(d{↓} F)}I\circ\pi\Too{{Π}_F(I)(g)}\lim_{(d'{↓} F)}I\circ\pi'.$$
\end{blockENG}

\begin{blockRUS}
\end{blockRUS}

\end{document}
